# ğŸ¯ æ¥ä¸‹ä¾†è¦åšä»€éº¼ï¼Ÿ

## âœ… å·²å®Œæˆçš„å·¥ä½œ

1. âœ… **æ¸…ç†èˆŠçˆ¬èŸ²** - åˆªé™¤ 10 å€‹æˆ¿åœ°ç”¢çˆ¬èŸ²
2. âœ… **æ›´æ–°é…ç½®** - config.pyã€.envã€docker-compose.yml
3. âœ… **å‰µå»º NPB çˆ¬èŸ²** - åŒ…å«å®Œæ•´çš„ç¯„æœ¬å’Œ TODO è¨»è§£
4. âœ… **æ›´æ–°æ¸¬è©¦æ–‡ä»¶** - test_crawler.py

## ğŸš§ NPB çˆ¬èŸ²éœ€è¦ä½ å®Œæˆçš„éƒ¨åˆ†

æˆ‘å‰µå»ºçš„ NPB çˆ¬èŸ²æ˜¯ä¸€å€‹**æ¨¡æ¿**ï¼ŒåŒ…å«äº†æ¶æ§‹å’Œé‚è¼¯ï¼Œä½†éœ€è¦ä½ æ ¹æ“šå¯¦éš›çš„ Yahoo Sports ç¶²ç«™çµæ§‹å¡«å……ç´°ç¯€ã€‚

### ğŸ“ éœ€è¦èª¿æ•´çš„åœ°æ–¹ï¼ˆæœ‰ TODO æ¨™è¨˜ï¼‰

åœ¨ `app/services/crawler/npb_crawler.py` ä¸­ï¼š

#### 1. **åˆ—è¡¨é  URL** (ç¬¬ 37-42 è¡Œ)
```python
# TODO: æ ¹æ“šå¯¦éš›ç¶²ç«™çµæ§‹èª¿æ•´ URL
list_url = f"{self.base_url}news/"
```

**ä½ éœ€è¦åšï¼š**
1. åœ¨ç€è¦½å™¨æ‰“é–‹ `https://baseball.yahoo.co.jp/npb/`
2. æ‰¾åˆ°æ–°èåˆ—è¡¨é é¢
3. ç¢ºèª URL æ ¼å¼ï¼ˆä¾‹å¦‚ï¼š`/npb/news/`ã€`/npb/articles/` ç­‰ï¼‰
4. æ›´æ–°ç¨‹å¼ç¢¼

---

#### 2. **åˆ—è¡¨é é¸æ“‡å™¨** (ç¬¬ 53-64 è¡Œ)
```python
# TODO: æ ¹æ“šå¯¦éš›ç¶²ç«™çµæ§‹èª¿æ•´é¸æ“‡å™¨
articles_html = soup.select('.bb-newsList__item')  # â† é€™è£¡éœ€è¦èª¿æ•´
```

**ä½ éœ€è¦åšï¼š**
1. åœ¨åˆ—è¡¨é æŒ‰ F12 æ‰“é–‹é–‹ç™¼è€…å·¥å…·
2. æª¢æŸ¥æ–‡ç« åˆ—è¡¨çš„ HTML çµæ§‹
3. æ‰¾åˆ°åŒ…å«æ–‡ç« é …ç›®çš„ CSS é¡åæˆ–æ¨™ç±¤
4. æ›´æ–°é¸æ“‡å™¨

**å¸¸è¦‹çš„é¸æ“‡å™¨ç¯„ä¾‹ï¼š**
- `.news-list .news-item`
- `article.article-card`
- `.bb-newsList__item`
- `div[class*="news"]`

---

#### 3. **æå–æ¨™é¡Œã€URLã€åœ–ç‰‡** (ç¬¬ 72-85 è¡Œ)
```python
# TODO: æ ¹æ“šå¯¦éš› HTML çµæ§‹èª¿æ•´ä»¥ä¸‹é¸æ“‡å™¨
title_elem = item.select_one('.bb-newsList__title')  # ç¯„ä¾‹
url = link_elem.get('href') if link_elem else None
img_elem = item.select_one('img')
```

**ä½ éœ€è¦åšï¼š**
1. åœ¨æ¯å€‹æ–‡ç« é …ç›®ä¸­æ‰¾åˆ°ï¼š
   - æ¨™é¡Œå…ƒç´ ï¼ˆé€šå¸¸æ˜¯ `<h2>`ã€`<h3>` æˆ–æœ‰ç‰¹å®š class çš„ `<a>`ï¼‰
   - URL é€£çµï¼ˆé€šå¸¸æ˜¯ `<a href="...">`ï¼‰
   - åœ–ç‰‡ï¼ˆé€šå¸¸æ˜¯ `<img src="...">`ï¼‰
2. æ›´æ–°é¸æ“‡å™¨

---

#### 4. **æ–‡ç« å…§å®¹é é¸æ“‡å™¨** (ç¬¬ 157-180 è¡Œ)
```python
# TODO: æ ¹æ“šå¯¦éš›ç¶²ç«™çµæ§‹èª¿æ•´é¸æ“‡å™¨
content_elem = soup.select_one('.bb-articleText')  # â† æ ¹æ“šå¯¦éš›ç¶²ç«™èª¿æ•´
```

**ä½ éœ€è¦åšï¼š**
1. é»æ“Šé€²å…¥ä¸€ç¯‡æ–‡ç« 
2. åœ¨æ–‡ç« é æŒ‰ F12
3. æ‰¾åˆ°åŒ…å«æ–‡ç« å…§å®¹çš„å…ƒç´ 
4. æ›´æ–°é¸æ“‡å™¨

**å¸¸è¦‹çš„å…§å®¹é¸æ“‡å™¨ï¼š**
- `.article-body`
- `.article-content`
- `.bb-articleText`
- `article .content`

---

## ğŸ› ï¸ èª¿è©¦æ­¥é©Ÿ

### æ–¹æ³• 1ï¼šç›´æ¥é‹è¡Œçˆ¬èŸ²ï¼ˆæ¨è–¦å…ˆç”¨é€™å€‹ï¼‰

```bash
# 1. å•Ÿå‹• Docker
docker-compose up -d

# 2. é‹è¡Œçˆ¬èŸ²ï¼ˆæœƒè‡ªå‹•å•Ÿå‹• Chrome ä¸¦é–‹å§‹çˆ¬å–ï¼‰
docker exec sportsnavi-web python -m app.tests.test_crawler npb \
    --start_date 2025-11-01 \
    --end_date 2025-11-03

# 3. æŸ¥çœ‹æ—¥èªŒ
docker logs -f sportsnavi-web
```

---

### æ–¹æ³• 2ï¼šå‰µå»ºèª¿è©¦è…³æœ¬ï¼ˆå»ºè­°åœ¨èª¿æ•´é¸æ“‡å™¨æ™‚ä½¿ç”¨ï¼‰

å‰µå»º `debug_npb.py`ï¼š

```python
from app.services.crawler.npb_crawler import NPBCrawler
import asyncio

async def debug():
    crawler = NPBCrawler()
    crawler.setup_driver()

    try:
        # æ¸¬è©¦åˆ—è¡¨é 
        print("=== æ¸¬è©¦åˆ—è¡¨é  ===")
        url = "https://baseball.yahoo.co.jp/npb/news/"
        crawler.wait_and_get(url)

        # æ‰“å°é é¢ HTMLï¼ˆå‰ 2000 å­—ï¼‰
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(crawler.driver.page_source, 'html.parser')
        print(soup.prettify()[:2000])

        # å˜—è©¦æå–æ–‡ç« åˆ—è¡¨
        articles = soup.select('.bb-newsList__item')  # â† èª¿æ•´é€™è£¡
        print(f"\næ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

        if articles:
            # æ‰“å°ç¬¬ä¸€ç¯‡æ–‡ç« çš„ HTML
            print("\n=== ç¬¬ä¸€ç¯‡æ–‡ç«  ===")
            print(articles[0].prettify())

    finally:
        crawler.cleanup()

if __name__ == '__main__':
    asyncio.run(debug())
```

é‹è¡Œï¼š
```bash
docker exec sportsnavi-web python debug_npb.py
```

---

### æ–¹æ³• 3ï¼šä½¿ç”¨ç€è¦½å™¨é–‹ç™¼è€…å·¥å…·

1. **æ‰“é–‹ç¶²ç«™**ï¼š`https://baseball.yahoo.co.jp/npb/`
2. **æŒ‰ F12** æ‰“é–‹é–‹ç™¼è€…å·¥å…·
3. **é»æ“Šé¸æ“‡å™¨åœ–æ¨™**ï¼ˆElement Selectorï¼‰
4. **é»æ“Šé é¢ä¸Šçš„æ–‡ç« æ¨™é¡Œ**
5. **æŸ¥çœ‹ HTML çµæ§‹**

ç¯„ä¾‹ï¼š
```html
<div class="bb-newsList__item">
    <a href="/npb/news/123">
        <h3 class="bb-newsList__title">æ–‡ç« æ¨™é¡Œ</h3>
        <img src="image.jpg" class="bb-newsList__image">
        <time class="bb-newsList__date">2025-11-03</time>
    </a>
</div>
```

æ ¹æ“šé€™å€‹çµæ§‹ï¼Œä½ éœ€è¦æ›´æ–°ï¼š
```python
articles_html = soup.select('.bb-newsList__item')
title_elem = item.select_one('.bb-newsList__title')
date_elem = item.select_one('.bb-newsList__date')
```

---

## ğŸ“‹ å®Œæ•´æ¸¬è©¦æµç¨‹

### æ­¥é©Ÿ 1ï¼šå•Ÿå‹•æœå‹™
```bash
cd /Users/maki/GitHub/sportsnavi
docker-compose up --build -d
```

### æ­¥é©Ÿ 2ï¼šæª¢æŸ¥æœå‹™ç‹€æ…‹
```bash
docker ps
# æ‡‰è©²çœ‹åˆ° sportsnavi-web å’Œ sportsnavi-db éƒ½åœ¨é‹è¡Œ
```

### æ­¥é©Ÿ 3ï¼šæ¸¬è©¦çˆ¬èŸ²
```bash
# å…ˆæ¸¬è©¦ 1-2 å¤©çš„è³‡æ–™
docker exec sportsnavi-web python -m app.tests.test_crawler npb \
    --start_date 2025-11-01 \
    --end_date 2025-11-02 \
    --debug  # é–‹å•Ÿ debug æ¨¡å¼æŸ¥çœ‹æ›´å¤šæ—¥èªŒ
```

### æ­¥é©Ÿ 4ï¼šæŸ¥çœ‹çµæœ

#### æ–¹æ³• Aï¼šæŸ¥çœ‹æ—¥èªŒ
```bash
docker logs sportsnavi-web --tail 100
```

#### æ–¹æ³• Bï¼šæŸ¥çœ‹è³‡æ–™åº«
```bash
# é€²å…¥è³‡æ–™åº«å®¹å™¨
docker exec -it sportsnavi-db psql -U user -d sportsnavidb

# æŸ¥è©¢è³‡æ–™
SELECT id, title, source, published_at
FROM articles
WHERE source='npb'
ORDER BY published_at DESC
LIMIT 10;

# é€€å‡º
\q
```

#### æ–¹æ³• Cï¼šä½¿ç”¨ Web UI
```bash
# è¨ªå•
http://localhost:8000
```

---

## âš ï¸ å¸¸è¦‹å•é¡Œ

### å•é¡Œ 1ï¼šçˆ¬èŸ²æ‰¾ä¸åˆ°æ–‡ç« 
**ç—‡ç‹€**ï¼šæ—¥èªŒé¡¯ç¤ºã€Œç¬¬ 1 é æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨ã€

**è§£æ±ºæ–¹æ³•**ï¼š
1. åˆ—è¡¨é é¸æ“‡å™¨ä¸æ­£ç¢º
2. ä½¿ç”¨ `debug_npb.py` æ‰“å° HTML
3. åœ¨ç€è¦½å™¨é–‹ç™¼è€…å·¥å…·ä¸­ç¢ºèªé¸æ“‡å™¨

---

### å•é¡Œ 2ï¼šæ–‡ç« å…§å®¹ç‚ºç©º
**ç—‡ç‹€**ï¼šæ—¥èªŒé¡¯ç¤ºã€Œæ–‡ç« å…§å®¹ç‚ºç©ºã€

**è§£æ±ºæ–¹æ³•**ï¼š
1. å…§å®¹é¸æ“‡å™¨ä¸æ­£ç¢º
2. é€²å…¥å¯¦éš›æ–‡ç« é é¢æª¢æŸ¥ HTML çµæ§‹
3. æ›´æ–° `content_elem` çš„é¸æ“‡å™¨

---

### å•é¡Œ 3ï¼šæ—¥æœŸè§£æå¤±æ•—
**ç—‡ç‹€**ï¼šæ—¥èªŒé¡¯ç¤ºã€Œç„¡æ³•è§£ææ—¥æœŸã€

**è§£æ±ºæ–¹æ³•**ï¼š
1. æª¢æŸ¥æ—¥æœŸå…ƒç´ çš„å¯¦éš›æ ¼å¼
2. å¦‚æœæ˜¯æ—¥æ–‡æ—¥æœŸï¼ˆä¾‹å¦‚ï¼šã€Œ2025å¹´11æœˆ3æ—¥ã€ï¼‰
3. åœ¨ `base.py` çš„ `parse_flexible_date` ä¸­æ–°å¢å°æ‡‰æ ¼å¼ï¼š

```python
# æ–°å¢æ—¥æ–‡æ—¥æœŸæ ¼å¼
'%Yå¹´%mæœˆ%dæ—¥',          # 2025å¹´11æœˆ3æ—¥
'%Yå¹´%mæœˆ%dæ—¥ %H:%M',    # 2025å¹´11æœˆ3æ—¥ 12:00
```

---

### å•é¡Œ 4ï¼šDocker è¨˜æ†¶é«”ä¸è¶³
**ç—‡ç‹€**ï¼šChrome ç„¡æ³•å•Ÿå‹•æˆ–å´©æ½°

**è§£æ±ºæ–¹æ³•**ï¼š
```bash
# æª¢æŸ¥ Docker è¨˜æ†¶é«”è¨­å®š
docker stats

# å¦‚æœä¸å¤ ï¼Œèª¿æ•´ docker-compose.yml
# é™ä½ web æœå‹™çš„è¨˜æ†¶é«”é™åˆ¶ï¼š
deploy:
  resources:
    limits:
      memory: 4G  # å¾ 6G é™åˆ° 4G
```

---

## âœ… å®Œæˆæª¢æŸ¥æ¸…å–®

åœ¨æ¸¬è©¦é€šéå¾Œï¼Œç¢ºèªï¼š

- [ ] åˆ—è¡¨é èƒ½æ­£ç¢ºè¼‰å…¥
- [ ] èƒ½æ‰¾åˆ°æ–‡ç« åˆ—è¡¨ï¼ˆè‡³å°‘ 1 ç¯‡ï¼‰
- [ ] æ¨™é¡Œæå–æ­£ç¢º
- [ ] URL æå–æ­£ç¢ºä¸”å®Œæ•´
- [ ] èƒ½é€²å…¥æ–‡ç« é é¢
- [ ] å…§å®¹æå–å®Œæ•´ï¼ˆä¸ç‚ºç©ºï¼‰
- [ ] æ—¥æœŸè§£ææˆåŠŸ
- [ ] è³‡æ–™æˆåŠŸå­˜å…¥è³‡æ–™åº«
- [ ] Web UI èƒ½çœ‹åˆ°æ–‡ç« 

---

## ğŸ‰ æ¸¬è©¦æˆåŠŸå¾Œ

æ­å–œï¼ä½ å·²ç¶“å®Œæˆæ ¸å¿ƒåŠŸèƒ½ã€‚æ¥ä¸‹ä¾†å¯ä»¥ï¼š

### é¸é … Aï¼šç¹¼çºŒå¯¦ä½œå…¶ä»–çˆ¬èŸ²
```bash
# è¤‡è£½ npb_crawler.py
cp app/services/crawler/npb_crawler.py app/services/crawler/mlb_crawler.py

# ä¿®æ”¹é¡åå’Œ URL
class MLBCrawler(BaseCrawler):
    def __init__(self):
        self.source_name = "MLB"
        self.base_url = "https://baseball.yahoo.co.jp/mlb/"
```

### é¸é … Bï¼šå¯¦ä½œé€²éšåŠŸèƒ½
- è³‡æ–™ä¿ç•™ç­–ç•¥ï¼ˆ13 å€‹æœˆè‡ªå‹•æ¸…ç†ï¼‰
- æ’ç¨‹å™¨ï¼ˆæ¯å¤© 4 æ¬¡è‡ªå‹•çˆ¬å–ï¼‰
- è¨˜æ†¶é«”ç›£æ§

### é¸é … Cï¼šå„ªåŒ–ç¾æœ‰çˆ¬èŸ²
- æ–°å¢éŒ¯èª¤è™•ç†
- æå‡çˆ¬å–é€Ÿåº¦
- æ–°å¢æ›´å¤šæ—¥æœŸæ ¼å¼æ”¯æ´

---

## ğŸ“ éœ€è¦å”åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°å•é¡Œï¼Œè«‹æä¾›ï¼š
1. å®Œæ•´çš„éŒ¯èª¤è¨Šæ¯
2. çˆ¬èŸ²æ—¥èªŒï¼ˆ`docker logs sportsnavi-web`ï¼‰
3. ç›®æ¨™ç¶²ç«™çš„ HTML çµæ§‹ï¼ˆé–‹ç™¼è€…å·¥å…·æˆªåœ–ï¼‰

æˆ‘å¯ä»¥å¹«ä½ èª¿æ•´é¸æ“‡å™¨æˆ–è§£æ±ºå…·é«”å•é¡Œï¼

---

**ç¾åœ¨ï¼Œè«‹å…ˆåŸ·è¡Œæ­¥é©Ÿ 1-3ï¼Œçœ‹çœ‹èƒ½å¦æˆåŠŸé‹è¡Œçˆ¬èŸ²ã€‚æœ‰ä»»ä½•å•é¡Œéš¨æ™‚å‘Šè¨´æˆ‘ï¼** ğŸš€
