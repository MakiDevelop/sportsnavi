# è³‡æ–™ä¿ç•™ç­–ç•¥ï¼ˆ13 å€‹æœˆï¼‰

## æ”¿ç­–èªªæ˜

- **ä¿ç•™æœŸé™**ï¼š13 å€‹æœˆ
- **æ¸…ç†é »ç‡**ï¼šæ¯æœˆ 1 è™Ÿå‡Œæ™¨ 3:00
- **æ¸…ç†æ¨™æº–**ï¼š`published_at` æ—©æ–¼ 13 å€‹æœˆå‰çš„æ–‡ç« 
- **å‚™ä»½ç­–ç•¥**ï¼šæ¸…ç†å‰å…ˆå‚™ä»½ï¼ˆå¯é¸ï¼‰

## å¯¦ä½œæ–¹æ¡ˆ

### 1. è³‡æ–™åº«æ¸…ç†å‡½æ•¸

```python
# app/services/data_retention.py

from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import func, delete
import logging

from app.models.article import Article
from app.core.database import SessionLocal

logger = logging.getLogger(__name__)

class DataRetentionService:
    """è³‡æ–™ä¿ç•™æœå‹™"""

    RETENTION_MONTHS = 13

    @staticmethod
    def calculate_cutoff_date() -> datetime:
        """è¨ˆç®—åˆªé™¤åˆ†ç•Œæ—¥æœŸ"""
        now = datetime.now()
        cutoff = now - timedelta(days=RETENTION_MONTHS * 30)
        return cutoff

    @staticmethod
    def get_articles_to_delete(db: Session) -> int:
        """æŸ¥è©¢å¾…åˆªé™¤æ–‡ç« æ•¸é‡"""
        cutoff_date = DataRetentionService.calculate_cutoff_date()

        count = db.query(func.count(Article.id)).filter(
            Article.published_at < cutoff_date
        ).scalar()

        return count

    @staticmethod
    def get_old_articles_summary(db: Session) -> dict:
        """å–å¾—å¾…åˆªé™¤æ–‡ç« çš„çµ±è¨ˆæ‘˜è¦"""
        cutoff_date = DataRetentionService.calculate_cutoff_date()

        # æŒ‰ä¾†æºçµ±è¨ˆ
        summary = db.query(
            Article.source,
            func.count(Article.id).label('count')
        ).filter(
            Article.published_at < cutoff_date
        ).group_by(Article.source).all()

        total = sum(item.count for item in summary)

        return {
            'total': total,
            'cutoff_date': cutoff_date,
            'by_source': [{'source': s.source, 'count': s.count} for s in summary]
        }

    @staticmethod
    def delete_old_articles(db: Session, dry_run: bool = False) -> dict:
        """
        åˆªé™¤è¶…éä¿ç•™æœŸé™çš„æ–‡ç« 

        Args:
            db: è³‡æ–™åº« session
            dry_run: True æ™‚åªçµ±è¨ˆä¸åˆªé™¤ï¼ˆæ¸¬è©¦ç”¨ï¼‰

        Returns:
            {
                'deleted_count': 123,
                'cutoff_date': '2024-10-03',
                'dry_run': False,
                'by_source': [...]
            }
        """
        cutoff_date = DataRetentionService.calculate_cutoff_date()

        # å–å¾—åˆªé™¤æ‘˜è¦
        summary = DataRetentionService.get_old_articles_summary(db)

        logger.info(
            f"{'[æ¸¬è©¦æ¨¡å¼] ' if dry_run else ''}æº–å‚™åˆªé™¤ {summary['total']} ç¯‡æ–‡ç« "
            f"ï¼ˆæ—©æ–¼ {cutoff_date.strftime('%Y-%m-%d')}ï¼‰"
        )

        for item in summary['by_source']:
            logger.info(f"  - {item['source']}: {item['count']} ç¯‡")

        if dry_run:
            logger.info("ğŸ” æ¸¬è©¦æ¨¡å¼ï¼šä¸å¯¦éš›åˆªé™¤")
            return {
                'deleted_count': 0,
                'cutoff_date': cutoff_date.strftime('%Y-%m-%d'),
                'dry_run': True,
                'would_delete': summary['total'],
                'by_source': summary['by_source']
            }

        # å¯¦éš›åˆªé™¤
        try:
            stmt = delete(Article).where(Article.published_at < cutoff_date)
            result = db.execute(stmt)
            deleted_count = result.rowcount

            db.commit()

            logger.info(f"âœ… æˆåŠŸåˆªé™¤ {deleted_count} ç¯‡æ–‡ç« ")

            return {
                'deleted_count': deleted_count,
                'cutoff_date': cutoff_date.strftime('%Y-%m-%d'),
                'dry_run': False,
                'by_source': summary['by_source']
            }

        except Exception as e:
            db.rollback()
            logger.error(f"âŒ åˆªé™¤å¤±æ•—: {str(e)}")
            raise

    @staticmethod
    def cleanup_with_backup(db: Session, backup_dir: str = None) -> dict:
        """
        åˆªé™¤å‰å…ˆå‚™ä»½

        Args:
            backup_dir: å‚™ä»½ç›®éŒ„è·¯å¾‘ï¼ŒNone è¡¨ç¤ºä¸å‚™ä»½
        """
        if backup_dir:
            # åŒ¯å‡ºåˆ° CSV
            cutoff_date = DataRetentionService.calculate_cutoff_date()

            from pathlib import Path
            import csv

            backup_path = Path(backup_dir)
            backup_path.mkdir(parents=True, exist_ok=True)

            backup_file = backup_path / f"articles_backup_{cutoff_date.strftime('%Y%m%d')}.csv"

            # æŸ¥è©¢å¾…åˆªé™¤çš„æ–‡ç« 
            old_articles = db.query(Article).filter(
                Article.published_at < cutoff_date
            ).all()

            # å¯«å…¥ CSV
            with open(backup_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['id', 'url', 'title', 'source', 'category', 'published_at', 'created_at'])

                for article in old_articles:
                    writer.writerow([
                        article.id,
                        article.url,
                        article.title,
                        article.source,
                        article.category,
                        article.published_at,
                        article.created_at
                    ])

            logger.info(f"ğŸ’¾ å·²å‚™ä»½ {len(old_articles)} ç¯‡æ–‡ç« åˆ° {backup_file}")

        # åŸ·è¡Œåˆªé™¤
        return DataRetentionService.delete_old_articles(db, dry_run=False)


def cleanup_old_data(dry_run: bool = False):
    """
    ä¸»è¦æ¸…ç†å‡½æ•¸ï¼ˆä¾›æ’ç¨‹å™¨å‘¼å«ï¼‰
    """
    db = SessionLocal()
    try:
        result = DataRetentionService.delete_old_articles(db, dry_run=dry_run)
        return result
    finally:
        db.close()
```

### 2. æ’ç¨‹å™¨é…ç½®

```python
# app/main.py

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from pytz import timezone

from app.services.data_retention import cleanup_old_data

def setup_scheduler():
    """è¨­å®šå®šæ™‚ä»»å‹™"""

    scheduler = AsyncIOScheduler(timezone=timezone('Asia/Taipei'))

    # çˆ¬èŸ²ä»»å‹™ï¼ˆæ¯å¤© 4 æ¬¡ï¼‰
    for hour in [8, 12, 16, 20]:
        scheduler.add_job(
            crawl_today,
            CronTrigger(hour=hour, minute=0),
            id=f'crawl_{hour}',
            replace_existing=True
        )

    # è³‡æ–™æ¸…ç†ä»»å‹™ï¼ˆæ¯æœˆ 1 è™Ÿå‡Œæ™¨ 3:00ï¼‰
    scheduler.add_job(
        cleanup_old_data,
        CronTrigger(day=1, hour=3, minute=0),
        id='data_cleanup',
        replace_existing=True,
        kwargs={'dry_run': False}
    )

    scheduler.start()
    logger.info("âœ… æ’ç¨‹å™¨å·²å•Ÿå‹•")
    logger.info("  - çˆ¬èŸ²ä»»å‹™: æ¯å¤© 8:00, 12:00, 16:00, 20:00")
    logger.info("  - è³‡æ–™æ¸…ç†: æ¯æœˆ 1 è™Ÿ 03:00")

    return scheduler
```

### 3. æ‰‹å‹•æ¸…ç†å‘½ä»¤ï¼ˆæ¸¬è©¦ç”¨ï¼‰

```python
# app/tests/test_cleanup.py

import argparse
from app.services.data_retention import DataRetentionService
from app.core.database import SessionLocal

def main():
    parser = argparse.ArgumentParser(description='æ¸¬è©¦è³‡æ–™æ¸…ç†')
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='æ¸¬è©¦æ¨¡å¼ï¼ˆä¸å¯¦éš›åˆªé™¤ï¼‰'
    )
    parser.add_argument(
        '--backup',
        type=str,
        help='å‚™ä»½ç›®éŒ„è·¯å¾‘'
    )
    parser.add_argument(
        '--summary',
        action='store_true',
        help='åªé¡¯ç¤ºçµ±è¨ˆï¼Œä¸åˆªé™¤'
    )

    args = parser.parse_args()

    db = SessionLocal()

    try:
        if args.summary:
            # åªé¡¯ç¤ºçµ±è¨ˆ
            summary = DataRetentionService.get_old_articles_summary(db)
            print(f"\nğŸ“Š å¾…åˆªé™¤æ–‡ç« çµ±è¨ˆ")
            print(f"ç¸½è¨ˆ: {summary['total']} ç¯‡")
            print(f"åˆ†ç•Œæ—¥æœŸ: {summary['cutoff_date'].strftime('%Y-%m-%d')}")
            print(f"\nå„ä¾†æºåˆ†å¸ƒ:")
            for item in summary['by_source']:
                print(f"  - {item['source']}: {item['count']} ç¯‡")

        elif args.backup:
            # å‚™ä»½å¾Œåˆªé™¤
            result = DataRetentionService.cleanup_with_backup(db, args.backup)
            print(f"\nâœ… æ¸…ç†å®Œæˆ")
            print(f"å·²åˆªé™¤: {result['deleted_count']} ç¯‡")
            print(f"åˆ†ç•Œæ—¥æœŸ: {result['cutoff_date']}")

        else:
            # ä¸€èˆ¬åˆªé™¤
            result = DataRetentionService.delete_old_articles(db, dry_run=args.dry_run)

            if args.dry_run:
                print(f"\nğŸ” æ¸¬è©¦æ¨¡å¼")
                print(f"å°‡åˆªé™¤: {result['would_delete']} ç¯‡")
            else:
                print(f"\nâœ… å·²åˆªé™¤: {result['deleted_count']} ç¯‡")

            print(f"åˆ†ç•Œæ—¥æœŸ: {result['cutoff_date']}")

    finally:
        db.close()

if __name__ == '__main__':
    main()
```

### 4. Docker ä½¿ç”¨ç¯„ä¾‹

```bash
# 1. æŸ¥çœ‹å¾…åˆªé™¤æ–‡ç« çµ±è¨ˆï¼ˆä¸åˆªé™¤ï¼‰
docker exec sportsnavi-web python -m app.tests.test_cleanup --summary

# è¼¸å‡ºç¯„ä¾‹ï¼š
# ğŸ“Š å¾…åˆªé™¤æ–‡ç« çµ±è¨ˆ
# ç¸½è¨ˆ: 1,234 ç¯‡
# åˆ†ç•Œæ—¥æœŸ: 2023-10-03
#
# å„ä¾†æºåˆ†å¸ƒ:
#   - npb: 456 ç¯‡
#   - mlb: 389 ç¯‡
#   - hsb: 389 ç¯‡

# 2. æ¸¬è©¦æ¨¡å¼ï¼ˆä¸å¯¦éš›åˆªé™¤ï¼‰
docker exec sportsnavi-web python -m app.tests.test_cleanup --dry-run

# 3. å¯¦éš›åˆªé™¤
docker exec sportsnavi-web python -m app.tests.test_cleanup

# 4. å‚™ä»½å¾Œåˆªé™¤
docker exec sportsnavi-web python -m app.tests.test_cleanup --backup /app/backups
```

### 5. ç›£æ§èˆ‡å‘Šè­¦

```python
# app/services/data_retention.py (æ–°å¢)

class DataRetentionMonitor:
    """ç›£æ§è³‡æ–™ä¿ç•™ç‹€æ…‹"""

    @staticmethod
    def get_retention_stats(db: Session) -> dict:
        """å–å¾—è³‡æ–™ä¿ç•™çµ±è¨ˆ"""

        # æœ€èˆŠæ–‡ç« 
        oldest = db.query(Article).order_by(Article.published_at).first()

        # æœ€æ–°æ–‡ç« 
        newest = db.query(Article).order_by(Article.published_at.desc()).first()

        # ç¸½æ–‡ç« æ•¸
        total = db.query(func.count(Article.id)).scalar()

        # è³‡æ–™åº«å¤§å°ï¼ˆè¿‘ä¼¼ï¼‰
        # PostgreSQL: SELECT pg_size_pretty(pg_total_relation_size('articles'));

        return {
            'total_articles': total,
            'oldest_article': oldest.published_at if oldest else None,
            'newest_article': newest.published_at if newest else None,
            'date_range_days': (newest.published_at - oldest.published_at).days if oldest and newest else 0
        }

    @staticmethod
    def check_retention_health(db: Session) -> dict:
        """æª¢æŸ¥è³‡æ–™ä¿ç•™æ˜¯å¦å¥åº·"""

        stats = DataRetentionMonitor.get_retention_stats(db)

        # æª¢æŸ¥æ˜¯å¦è¶…é 13 å€‹æœˆ
        if stats['oldest_article']:
            age_days = (datetime.now() - stats['oldest_article']).days
            max_allowed_days = 13 * 30  # 390 å¤©

            if age_days > max_allowed_days:
                return {
                    'healthy': False,
                    'reason': f'è³‡æ–™è¶…éä¿ç•™æœŸé™ ({age_days} å¤© > {max_allowed_days} å¤©)',
                    'action': 'éœ€è¦åŸ·è¡Œæ¸…ç†',
                    'stats': stats
                }

        return {
            'healthy': True,
            'stats': stats
        }
```

### 6. API ç«¯é»ï¼ˆå¯é¸ï¼‰

```python
# app/api/admin.py

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session

from app.core.database import get_db
from app.services.data_retention import DataRetentionService, DataRetentionMonitor

router = APIRouter(prefix="/api/admin", tags=["admin"])

@router.get("/retention/stats")
def get_retention_stats(db: Session = Depends(get_db)):
    """å–å¾—è³‡æ–™ä¿ç•™çµ±è¨ˆ"""
    return DataRetentionMonitor.get_retention_stats(db)

@router.get("/retention/health")
def check_retention_health(db: Session = Depends(get_db)):
    """æª¢æŸ¥è³‡æ–™ä¿ç•™å¥åº·ç‹€æ…‹"""
    return DataRetentionMonitor.check_retention_health(db)

@router.get("/retention/summary")
def get_cleanup_summary(db: Session = Depends(get_db)):
    """å–å¾—å¾…æ¸…ç†æ–‡ç« æ‘˜è¦"""
    return DataRetentionService.get_old_articles_summary(db)

@router.post("/retention/cleanup")
def trigger_cleanup(dry_run: bool = False, db: Session = Depends(get_db)):
    """æ‰‹å‹•è§¸ç™¼æ¸…ç†ï¼ˆéœ€è¦ç®¡ç†å“¡æ¬Šé™ï¼‰"""
    result = DataRetentionService.delete_old_articles(db, dry_run=dry_run)
    return result
```

## æ¸¬è©¦æª¢æŸ¥æ¸…å–®

- [ ] æ¸¬è©¦ `--summary` æŸ¥çœ‹çµ±è¨ˆ
- [ ] æ¸¬è©¦ `--dry-run` ä¸å¯¦éš›åˆªé™¤
- [ ] æ¸¬è©¦å¯¦éš›åˆªé™¤åŠŸèƒ½
- [ ] æ¸¬è©¦å‚™ä»½åŠŸèƒ½
- [ ] é©—è­‰æ’ç¨‹å™¨åœ¨æ¯æœˆ 1 è™ŸåŸ·è¡Œ
- [ ] æª¢æŸ¥åˆªé™¤å¾Œè³‡æ–™åº«å¤§å°è®ŠåŒ–

## æ³¨æ„äº‹é …

1. **é¦–æ¬¡åŸ·è¡Œå»ºè­°ä½¿ç”¨ `--dry-run`**ï¼Œç¢ºèªç„¡èª¤å¾Œå†å¯¦éš›åˆªé™¤
2. **å»ºè­°åœ¨ä½å³°æœŸåŸ·è¡Œ**ï¼ˆå‡Œæ™¨ 3:00ï¼‰
3. **é‡è¦ï¼šåˆªé™¤å‰å…ˆå‚™ä»½**ï¼ˆè‡³å°‘ä¿ç•™æœ€è¿‘ä¸€æ¬¡å‚™ä»½ï¼‰
4. **PostgreSQL VACUUM**ï¼šåˆªé™¤å¾ŒåŸ·è¡Œ `VACUUM FULL articles;` å›æ”¶ç©ºé–“

## é æœŸæ•ˆæœ

- **è³‡æ–™é‡æ§åˆ¶**ï¼šç¶­æŒåœ¨ ~6-8GBï¼ˆ13 å€‹æœˆè³‡æ–™ï¼‰
- **æŸ¥è©¢æ•ˆèƒ½**ï¼šåˆªé™¤èˆŠè³‡æ–™å¾ŒæŸ¥è©¢æ›´å¿«
- **å„²å­˜æˆæœ¬**ï¼šé¿å…ç„¡é™å¢é•·

**ç¸½çµ**ï¼šæ¯æœˆè‡ªå‹•æ¸…ç† + æ‰‹å‹•å‚™ä»½æ©Ÿåˆ¶ï¼Œç¢ºä¿è³‡æ–™ä¿ç•™åœ¨ 13 å€‹æœˆå…§ã€‚
