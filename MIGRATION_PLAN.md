# ğŸ”„ å°ˆæ¡ˆé·ç§»è¨ˆåŠƒï¼šå¾ REAS åˆ° Sportsnavi

## ğŸ“Š ç•¶å‰ç‹€æ³åˆ†æ

### âœ… å·²è¤‡è£½çš„æ¶æ§‹ï¼ˆä¿ç•™ï¼‰

```
sportsnavi/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/               âœ… æ ¸å¿ƒé…ç½®ï¼ˆéœ€ä¿®æ”¹ï¼‰
â”‚   â”‚   â”œâ”€â”€ config.py       â†’ æ›´æ–°ç‚º Yahoo Sports é…ç½®
â”‚   â”‚   â”œâ”€â”€ database.py     âœ… ä¿ç•™ä¸è®Š
â”‚   â”‚   â”œâ”€â”€ db_utils.py     âœ… ä¿ç•™ä¸è®Š
â”‚   â”‚   â””â”€â”€ logging_config.py âœ… ä¿ç•™ä¸è®Š
â”‚   â”œâ”€â”€ models/             âœ… è³‡æ–™æ¨¡å‹ï¼ˆå¯èƒ½éœ€å¾®èª¿ï¼‰
â”‚   â”‚   â””â”€â”€ article.py      â†’ æª¢æŸ¥æ¬„ä½æ˜¯å¦é©ç”¨
â”‚   â”œâ”€â”€ schemas/            âœ… Pydantic schemas
â”‚   â”‚   â””â”€â”€ article.py      âœ… ä¿ç•™ä¸è®Š
â”‚   â”œâ”€â”€ api/                âœ… REST API
â”‚   â”‚   â””â”€â”€ v1/             âœ… ä¿ç•™ä¸è®Š
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ crawler/
â”‚   â”‚       â””â”€â”€ base.py     âœ… ä¿ç•™ï¼ˆå¯èƒ½éœ€å¾®èª¿ï¼‰
â”‚   â”œâ”€â”€ templates/          âœ… Web UI
â”‚   â”œâ”€â”€ static/             âœ… éœæ…‹è³‡æº
â”‚   â””â”€â”€ tests/              â†’ éœ€æ›´æ–°æ¸¬è©¦
â”œâ”€â”€ docker-compose.yml      âœ… ä¿ç•™ï¼ˆéœ€å¾®èª¿ï¼‰
â”œâ”€â”€ Dockerfile              âœ… ä¿ç•™ä¸è®Š
â”œâ”€â”€ requirements.txt        âœ… ä¿ç•™ä¸è®Š
â””â”€â”€ .env                    â†’ éœ€ä¿®æ”¹
```

### âŒ éœ€è¦æ¸…ç†çš„èˆŠçˆ¬èŸ²ï¼ˆæˆ¿åœ°ç”¢æ–°èï¼‰

```
app/services/crawler/
â”œâ”€â”€ bharian_crawler.py          âŒ åˆªé™¤ (é¦¬ä¾†è¥¿äºæˆ¿ç”¢)
â”œâ”€â”€ ebc_crawler.py              âŒ åˆªé™¤ (æ±æ£®æˆ¿ç”¢)
â”œâ”€â”€ edgeprop_crawler.py         âŒ åˆªé™¤ (EdgeProp æˆ¿ç”¢)
â”œâ”€â”€ ettoday_crawler.py          âŒ åˆªé™¤ (ETtoday æˆ¿ç”¢)
â”œâ”€â”€ freemalaysiatoday_crawler.py âŒ åˆªé™¤ (é¦¬ä¾†è¥¿äºæˆ¿ç”¢)
â”œâ”€â”€ hk852house_crawler.py       âŒ åˆªé™¤ (é¦™æ¸¯æˆ¿ç”¢)
â”œâ”€â”€ ltn_crawler.py              âŒ åˆªé™¤ (è‡ªç”±æ™‚å ±æˆ¿ç”¢)
â”œâ”€â”€ nextapple_crawler.py        âŒ åˆªé™¤ (è˜‹æœåœ°ç”¢)
â”œâ”€â”€ starproperty_crawler.py     âŒ åˆªé™¤ (Star Property æˆ¿ç”¢)
â””â”€â”€ udn_crawler.py              âŒ åˆªé™¤ (è¯åˆå ±æˆ¿ç”¢)
```

### âœ¨ éœ€è¦æ–°å¢çš„ Yahoo Sports çˆ¬èŸ²

```
app/services/crawler/
â”œâ”€â”€ base.py                      âœ… å·²å­˜åœ¨ï¼ˆä¿ç•™ï¼‰
â”œâ”€â”€ npb_crawler.py              ğŸ†• æ–°å¢ (æ—¥æœ¬è·æ£’)
â”œâ”€â”€ mlb_crawler.py              ğŸ†• æ–°å¢ (ç¾åœ‹å¤§è¯ç›Ÿ)
â”œâ”€â”€ hsb_crawler.py              ğŸ†• æ–°å¢ (é«˜æ ¡é‡çƒ)
â”œâ”€â”€ bbl_crawler.py              ğŸ†• æ–°å¢ (å¤§å­¸é‡çƒ)
â”œâ”€â”€ ind_crawler.py              ğŸ†• æ–°å¢ (ç¨ç«‹è¯ç›Ÿ)
â””â”€â”€ amateur_crawler.py          ğŸ†• æ–°å¢ (æ¥­é¤˜æ£’çƒ)
```

---

## ğŸ¯ é·ç§»æ­¥é©Ÿ

### Phase 1: æ¸…ç†èˆŠçˆ¬èŸ² âœ‚ï¸

#### æ­¥é©Ÿ 1.1ï¼šåˆªé™¤èˆŠçˆ¬èŸ²æ–‡ä»¶

```bash
# åˆªé™¤æ‰€æœ‰æˆ¿åœ°ç”¢çˆ¬èŸ²
cd app/services/crawler/
rm bharian_crawler.py
rm ebc_crawler.py
rm edgeprop_crawler.py
rm ettoday_crawler.py
rm freemalaysiatoday_crawler.py
rm hk852house_crawler.py
rm ltn_crawler.py
rm nextapple_crawler.py
rm starproperty_crawler.py
rm udn_crawler.py

# ä¿ç•™
# - base.py
# - __init__.py
```

#### æ­¥é©Ÿ 1.2ï¼šæ¸…ç†å…¶ä»–ç›¸é—œæ–‡ä»¶

```bash
# åˆªé™¤ debug æ–‡ä»¶
rm debug_ettoday.py
rm debug_ettoday_article.html
rm debug_ettoday_output.html

# æ¸…ç†æ—¥èªŒï¼ˆå¯é¸ï¼‰
rm -rf logs/*

# æ¸…ç†èˆŠçš„å„ªåŒ–æ–‡æª”ï¼ˆå¦‚æœæœ‰è¡çªï¼‰
# OPTIMIZATION_SUMMARY.md å¯ä»¥ä¿ç•™æˆ–åˆªé™¤
```

---

### Phase 2: æ›´æ–°é…ç½® âš™ï¸

#### æ­¥é©Ÿ 2.1ï¼šæ›´æ–° `app/core/config.py`

```python
# app/core/config.py

import os
from typing import Optional, Dict, Any, List
from pydantic_settings import BaseSettings
from pydantic import validator

class Settings(BaseSettings):
    # å°ˆæ¡ˆåŸºæœ¬è¨­å®š
    PROJECT_NAME: str = "Sportsnavi API"
    API_V1_STR: str = "/api/v1"
    BACKEND_CORS_ORIGINS: List[str] = ["*"]
    SECRET_KEY: str
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 8

    # Yahoo Sports ä¾†æºè¨­å®š
    NEWS_SOURCES: Dict[str, Dict[str, Any]] = {
        "npb": {
            "name": "æ—¥æœ¬è·æ£’ NPB",
            "base_url": "https://baseball.yahoo.co.jp/npb/"
        },
        "mlb": {
            "name": "ç¾åœ‹å¤§è¯ç›Ÿ MLB",
            "base_url": "https://baseball.yahoo.co.jp/mlb/"
        },
        "hsb": {
            "name": "é«˜æ ¡é‡çƒ",
            "base_url": "https://baseball.yahoo.co.jp/hsb/"
        },
        "bbl": {
            "name": "å¤§å­¸é‡çƒ",
            "base_url": "https://baseball.yahoo.co.jp/bbl/"
        },
        "ind": {
            "name": "ç¨ç«‹è¯ç›Ÿ",
            "base_url": "https://baseball.yahoo.co.jp/ind/"
        },
        "amateur": {
            "name": "æ¥­é¤˜æ£’çƒ",
            "base_url": "https://baseball.yahoo.co.jp/amateur/"
        }
    }

    # è³‡æ–™åº«è¨­å®š
    POSTGRES_USER: str = "user"
    POSTGRES_PASSWORD: str
    POSTGRES_SERVER: str = "db"
    POSTGRES_DB: str = "sportsnavidb"  # â† æ”¹å
    DATABASE_URL: Optional[str] = None

    # Chrome è¨­å®š
    CHROME_BIN: str = "/usr/bin/chromium"
    CHROMEDRIVER_PATH: str = "/usr/bin/chromedriver"

    # çˆ¬èŸ²è¨­å®š
    CRAWLER_MAX_RETRIES: int = 3
    CRAWLER_TIMEOUT: int = 30
    CRAWLER_DELAY_MIN: int = 1
    CRAWLER_DELAY_MAX: int = 3

    # è¨˜æ†¶é«”ç®¡ç†ï¼ˆæ–°å¢ï¼‰
    MAX_CONCURRENT_CRAWLERS: int = int(os.getenv('MAX_CONCURRENT_CRAWLERS', '3'))
    MAX_RAM_GB: int = 8
    RESERVED_RAM_GB: int = 2

    # è³‡æ–™ä¿ç•™è¨­å®šï¼ˆæ–°å¢ï¼‰
    RETENTION_MONTHS: int = 13

    # æ—¥èªŒè¨­å®š
    LOG_LEVEL: str = "INFO"

    @validator("DATABASE_URL", pre=True)
    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> str:
        if isinstance(v, str):
            return v
        return (
            f"postgresql://"
            f"{values.get('POSTGRES_USER')}:"
            f"{values.get('POSTGRES_PASSWORD')}@"
            f"{values.get('POSTGRES_SERVER')}/"
            f"{values.get('POSTGRES_DB')}"
        )

    @validator("SECRET_KEY")
    def validate_secret_key(cls, v: str) -> str:
        if v == "your-secret-key" or v == "your-secret-key-change-this-in-production":
            raise ValueError("Please change SECRET_KEY in production environment")
        if len(v) < 32:
            raise ValueError("SECRET_KEY must be at least 32 characters long")
        return v

    class Config:
        case_sensitive = True
        env_file = ".env"
        env_file_encoding = "utf-8"

settings = Settings()
```

#### æ­¥é©Ÿ 2.2ï¼šæ›´æ–° `.env` æ–‡ä»¶

```bash
# .env

# å°ˆæ¡ˆè¨­å®š
PROJECT_NAME="Sportsnavi API"
SECRET_KEY="your-secret-key-at-least-32-characters-long-change-this"

# è³‡æ–™åº«è¨­å®š
POSTGRES_USER=user
POSTGRES_PASSWORD=your_secure_password_here
POSTGRES_SERVER=db
POSTGRES_DB=sportsnavidb

# çˆ¬èŸ²è¨­å®š
MAX_CONCURRENT_CRAWLERS=3
CRAWLER_MAX_RETRIES=3
CRAWLER_TIMEOUT=30

# Chrome è¨­å®š
CHROME_HEADLESS=true
DISABLE_IMAGES=true

# æ—¥èªŒè¨­å®š
LOG_LEVEL=INFO
```

#### æ­¥é©Ÿ 2.3ï¼šæ›´æ–° `docker-compose.yml`

```yaml
# docker-compose.yml

version: '3.8'

services:
  db:
    image: postgres:15-alpine
    container_name: sportsnavi-db
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: sportsnavidb  # â† æ”¹å
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    deploy:
      resources:
        limits:
          memory: 1.5G
        reservations:
          memory: 512M
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=256MB"
      - "-c"
      - "max_connections=50"

  web:
    build: .
    container_name: sportsnavi-web
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - .:/app
    ports:
      - "8000:8000"
    depends_on:
      - db
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_SERVER=db
      - POSTGRES_DB=sportsnavidb
      - SECRET_KEY=${SECRET_KEY}
      - MAX_CONCURRENT_CRAWLERS=3
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 2G

volumes:
  postgres_data:
```

---

### Phase 3: å¯¦ä½œ Yahoo Sports çˆ¬èŸ² ğŸ—ï¸

#### æ­¥é©Ÿ 3.1ï¼šæª¢æŸ¥ `base.py` æ˜¯å¦éœ€è¦èª¿æ•´

å…ˆè®€å– `base.py`ï¼Œç¢ºèªæ˜¯å¦éœ€è¦ä¿®æ”¹ã€‚

#### æ­¥é©Ÿ 3.2ï¼šå‰µå»º NPB çˆ¬èŸ²ç¯„æœ¬

```python
# app/services/crawler/npb_crawler.py

from .base import BaseCrawler
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class NPBCrawler(BaseCrawler):
    """æ—¥æœ¬è·æ£’ (NPB) çˆ¬èŸ²"""

    def __init__(self):
        super().__init__()
        self.source_name = "NPB"
        self.base_url = "https://baseball.yahoo.co.jp/npb/"
        self.needs_javascript = True

    async def crawl_list(self, page: int = 1) -> List[Dict]:
        """
        çˆ¬å–åˆ—è¡¨é 

        Returns:
            [
                {
                    'title': 'æ–‡ç« æ¨™é¡Œ',
                    'url': 'æ–‡ç« URL',
                    'image_url': 'åœ–ç‰‡URL',
                    'category': 'NPB',
                },
                ...
            ]
        """
        # TODO: å¯¦ä½œåˆ—è¡¨é çˆ¬å–é‚è¼¯
        pass

    async def crawl_article(self, article_info: Dict) -> Optional[Dict]:
        """
        çˆ¬å–æ–‡ç« å…§å®¹

        Returns:
            {
                'title': 'æ–‡ç« æ¨™é¡Œ',
                'content': 'æ–‡ç« å…§å®¹',
                'description': 'æ‘˜è¦',
                'published_at': datetime,
                'image_url': 'åœ–ç‰‡URL',
                'category': 'NPB',
                'reporter': 'è¨˜è€…',
            }
        """
        # TODO: å¯¦ä½œæ–‡ç« å…§å®¹çˆ¬å–é‚è¼¯
        pass
```

#### æ­¥é©Ÿ 3.3ï¼šæ›´æ–° `test_crawler.py`

```python
# app/tests/test_crawler.py

import asyncio
import sys
from app.services.crawler.npb_crawler import NPBCrawler
from app.services.crawler.mlb_crawler import MLBCrawler
from app.services.crawler.hsb_crawler import HSBCrawler
from app.services.crawler.bbl_crawler import BBLCrawler
from app.services.crawler.ind_crawler import INDCrawler
from app.services.crawler.amateur_crawler import AmateurCrawler
from app.core.database import SessionLocal
from app.models.article import Article
import pytest
from datetime import datetime, timedelta
import argparse
import logging

# è¨­å®šæ—¥èªŒæ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

logger = logging.getLogger(__name__)

def get_crawler(crawler_name: str):
    """æ ¹æ“šåç¨±å–å¾—å°æ‡‰çš„çˆ¬èŸ²å¯¦ä¾‹"""
    crawlers = {
        'npb': NPBCrawler(),
        'mlb': MLBCrawler(),
        'hsb': HSBCrawler(),
        'bbl': BBLCrawler(),
        'ind': INDCrawler(),
        'amateur': AmateurCrawler(),
    }
    return crawlers.get(crawler_name)

@pytest.mark.asyncio
async def test_crawler(crawler_type="npb", start_date=None, end_date=None):
    """æ¸¬è©¦çˆ¬èŸ²"""
    try:
        # æ ¹æ“šåƒæ•¸é¸æ“‡çˆ¬èŸ²
        crawler = get_crawler(crawler_type.lower())
        if not crawler:
            raise ValueError(f"æœªçŸ¥çš„çˆ¬èŸ²é¡å‹: {crawler_type}")

        logger.info(f"é–‹å§‹çˆ¬å– {crawler_type} æ–‡ç«  (æ—¥æœŸç¯„åœ: {start_date} ~ {end_date})...")

        # åˆå§‹åŒ– driver
        if hasattr(crawler, 'setup_driver'):
            crawler.setup_driver()

        try:
            # åŸ·è¡Œçˆ¬èŸ²
            articles = await crawler.crawl(start_date=start_date, end_date=end_date)

            logger.info(f"çˆ¬å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")

            # å­˜å…¥è³‡æ–™åº«ï¼ˆä½¿ç”¨æ‰¹æ¬¡æ“ä½œï¼‰
            db = SessionLocal()
            try:
                from app.core.db_utils import batch_upsert_articles

                # æº–å‚™æ–‡ç« è³‡æ–™
                article_data_list = []
                for article in articles:
                    if isinstance(article, dict):
                        article_data = {
                            'url': article.get('url'),
                            'title': article.get('title'),
                            'content': article.get('content'),
                            'published_at': article.get('published_at'),
                            'source': crawler_type.lower(),
                            'image_url': article.get('image_url'),
                            'description': article.get('description'),
                            'category': article.get('category'),
                            'reporter': article.get('reporter'),
                        }
                    else:
                        article_data = {
                            'url': article.url,
                            'title': article.title,
                            'content': article.content,
                            'published_at': article.published_at,
                            'source': crawler_type.lower(),
                            'image_url': article.image_url,
                            'description': article.description,
                            'category': getattr(article, 'category', None),
                            'reporter': getattr(article, 'reporter', None),
                        }
                    article_data_list.append(article_data)

                # æ‰¹æ¬¡ upsert
                saved_count, updated_count = batch_upsert_articles(db, article_data_list, batch_size=50)

                logger.info(f"å®Œæˆï¼æ–°å¢: {saved_count} ç¯‡ï¼Œæ›´æ–°: {updated_count} ç¯‡")
                return len(articles)

            except Exception as e:
                logger.error(f"è³‡æ–™åº«æ“ä½œå¤±æ•—: {str(e)}")
                db.rollback()
                raise
            finally:
                db.close()

        finally:
            # æ¸…ç†è³‡æº
            if hasattr(crawler, 'cleanup'):
                crawler.cleanup()

    except Exception as e:
        logger.error(f"çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {str(e)}")
        raise

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('crawler',
                       choices=['npb', 'mlb', 'hsb', 'bbl', 'ind', 'amateur'],
                       help='æŒ‡å®šè¦æ¸¬è©¦çš„çˆ¬èŸ²')
    parser.add_argument('--start_date',
                       help='å›è£œèµ·å§‹æ—¥æœŸ (YYYY-MM-DD)',
                       default='2025-01-07')
    parser.add_argument('--end_date',
                       help='å›è£œçµæŸæ—¥æœŸ (YYYY-MM-DD)',
                       default='2025-01-07')
    parser.add_argument('--debug', action='store_true',
                       help='é–‹å•Ÿé™¤éŒ¯æ¨¡å¼')
    args = parser.parse_args()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    asyncio.run(test_crawler(args.crawler, args.start_date, args.end_date))
```

---

### Phase 4: å¯¦ä½œè³‡æ–™ä¿ç•™èˆ‡è¨˜æ†¶é«”ç®¡ç† ğŸ”§

#### æ­¥é©Ÿ 4.1ï¼šæ–°å¢è³‡æ–™ä¿ç•™æœå‹™

```bash
# å‰µå»ºæ–°æ–‡ä»¶
touch app/services/data_retention.py
```

å°‡ `data_retention_policy.md` ä¸­çš„ç¨‹å¼ç¢¼è¤‡è£½åˆ°é€™å€‹æ–‡ä»¶ã€‚

#### æ­¥é©Ÿ 4.2ï¼šæ›´æ–° `main.py` åŠ å…¥æ’ç¨‹å™¨

æ›´æ–° `app/main.py`ï¼ŒåŠ å…¥ï¼š
1. è³‡æ–™æ¸…ç†æ’ç¨‹ï¼ˆæ¯æœˆ 1 è™Ÿå‡Œæ™¨ 3:00ï¼‰
2. çˆ¬èŸ²æ’ç¨‹ï¼ˆæ¯å¤© 4 æ¬¡ï¼‰
3. ä¸¦ç™¼æ§åˆ¶ï¼ˆasyncio.Semaphoreï¼‰

---

### Phase 5: æ¸¬è©¦èˆ‡é©—è­‰ âœ…

#### æ­¥é©Ÿ 5.1ï¼šæ¸…ç†è³‡æ–™åº«ï¼ˆå¦‚æœéœ€è¦ï¼‰

```bash
# å¦‚æœè¦å…¨æ–°é–‹å§‹
docker-compose down -v  # åˆªé™¤æ‰€æœ‰ volumes
docker-compose up -d db  # åªå•Ÿå‹•è³‡æ–™åº«
```

#### æ­¥é©Ÿ 5.2ï¼šå•Ÿå‹•æœå‹™

```bash
docker-compose up --build -d
```

#### æ­¥é©Ÿ 5.3ï¼šæ¸¬è©¦çˆ¬èŸ²

```bash
# æ¸¬è©¦å–®å€‹çˆ¬èŸ²
docker exec sportsnavi-web python -m app.tests.test_crawler npb \
    --start_date 2025-11-01 --end_date 2025-11-03

# æ¸¬è©¦è³‡æ–™æ¸…ç†ï¼ˆdry-runï¼‰
docker exec sportsnavi-web python -m app.tests.test_cleanup --summary
```

---

## ğŸ“‹ åŸ·è¡Œæª¢æŸ¥æ¸…å–®

### Phase 1: æ¸…ç† âœ‚ï¸
- [ ] åˆªé™¤ 10 å€‹èˆŠçˆ¬èŸ²æ–‡ä»¶
- [ ] åˆªé™¤ debug æ–‡ä»¶
- [ ] æ¸…ç† logsï¼ˆå¯é¸ï¼‰

### Phase 2: é…ç½® âš™ï¸
- [ ] æ›´æ–° `config.py`
- [ ] æ›´æ–° `.env`
- [ ] æ›´æ–° `docker-compose.yml`
- [ ] æª¢æŸ¥ `base.py` æ˜¯å¦éœ€è¦èª¿æ•´

### Phase 3: å¯¦ä½œçˆ¬èŸ² ğŸ—ï¸
- [ ] å‰µå»º `npb_crawler.py`
- [ ] å‰µå»º `mlb_crawler.py`
- [ ] å‰µå»º `hsb_crawler.py`
- [ ] å‰µå»º `bbl_crawler.py`
- [ ] å‰µå»º `ind_crawler.py`
- [ ] å‰µå»º `amateur_crawler.py`
- [ ] æ›´æ–° `test_crawler.py`

### Phase 4: é€²éšåŠŸèƒ½ ğŸ”§
- [ ] å‰µå»º `data_retention.py`
- [ ] æ›´æ–° `main.py` åŠ å…¥æ’ç¨‹å™¨
- [ ] å‰µå»º `test_cleanup.py`

### Phase 5: æ¸¬è©¦ âœ…
- [ ] è³‡æ–™åº«é·ç§»ï¼ˆå¦‚éœ€è¦ï¼‰
- [ ] å•Ÿå‹•æœå‹™
- [ ] æ¸¬è©¦å–®å€‹çˆ¬èŸ²
- [ ] æ¸¬è©¦è³‡æ–™æ¸…ç†
- [ ] æ¸¬è©¦ Web UI

---

## âš ï¸ æ³¨æ„äº‹é …

1. **å‚™ä»½**ï¼šå¦‚æœ reas å°ˆæ¡ˆé‚„åœ¨ä½¿ç”¨ï¼Œå»ºè­°å…ˆå‚™ä»½
2. **è³‡æ–™åº«**ï¼šæ–°å°ˆæ¡ˆä½¿ç”¨æ–°çš„è³‡æ–™åº«åç¨±ï¼ˆ`sportsnavidb`ï¼‰
3. **æ¸¬è©¦**ï¼šæ¯å€‹éšæ®µå®Œæˆå¾Œéƒ½è¦æ¸¬è©¦
4. **æ¼¸é€²å¼**ï¼šä¸éœ€è¦ä¸€æ¬¡å¯¦ä½œæ‰€æœ‰ 6 å€‹çˆ¬èŸ²ï¼Œå¯ä»¥å…ˆåš NPB

---

## ğŸš€ å¿«é€Ÿé–‹å§‹ï¼ˆæœ€å°å¯è¡Œç‰ˆæœ¬ï¼‰

å¦‚æœæƒ³æœ€å¿«å•Ÿå‹•ï¼Œå¯ä»¥ï¼š

1. **åªæ¸…ç†çˆ¬èŸ²æ–‡ä»¶**ï¼ˆ10 å€‹èˆŠçˆ¬èŸ²ï¼‰
2. **åªæ›´æ–° config.py å’Œ .env**
3. **åªå¯¦ä½œ 1 å€‹çˆ¬èŸ²**ï¼ˆNPBï¼‰
4. **æ¸¬è©¦èƒ½å¦é‹è¡Œ**

å…¶ä»–åŠŸèƒ½ï¼ˆè³‡æ–™æ¸…ç†ã€è¨˜æ†¶é«”ç®¡ç†ã€å…¶ä»–çˆ¬èŸ²ï¼‰å¯ä»¥å¾ŒçºŒå†åŠ ã€‚

---

**é è¨ˆæ™‚é–“**ï¼š
- Phase 1-2: 30 åˆ†é˜
- Phase 3 (1 å€‹çˆ¬èŸ²): 2-4 å°æ™‚
- Phase 4: 1 å°æ™‚
- Phase 5: 30 åˆ†é˜

**ç¸½è¨ˆ**: ç´„ 4-6 å°æ™‚ï¼ˆå¯¦ä½œ 1 å€‹çˆ¬èŸ²çš„æƒ…æ³ï¼‰
